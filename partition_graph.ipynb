{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat reddit.csv | awk '{print $1, $2, 1}' > reddit.ijv\n",
    "!csrcnv reddit.ijv 6 reddit.adj 3\n",
    "!gpmetis reddit.adj 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as spsp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.loadtxt('reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm = spsp.coo_matrix((np.ones(mat.shape[0]), (mat[:,0].astype(np.int64),\n",
    "                                               mat[:,1].astype(np.int64))))\n",
    "spm = spm.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "spm = spsp.load_npz('reddit.npz')\n",
    "g = dgl.DGLGraph(spm, readonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parts = 4\n",
    "node_locs = np.loadtxt('reddit.adj.part.{}'.format(num_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = dgl.transform.partition_graph_with_halo(g, node_locs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import pickle\n",
    "\n",
    "print('#nodes: {}, #edges: {}'.format(g.number_of_nodes(), g.number_of_edges()))\n",
    "part_results = []\n",
    "for i, part in parts.items():\n",
    "    print('part:', i)\n",
    "    print('#nodes: {}, #inner nodes: {}'.format(part.number_of_nodes(),\n",
    "                                                th.sum(part.ndata['inner_node'])))\n",
    "    print('#edges: {}, #inner edges: {}'.format(part.number_of_edges(),\n",
    "                                                th.sum(part.edata['inner_edge'])))\n",
    "    out_spm = part.adjacency_matrix_scipy(transpose=True)\n",
    "    part_nodes = part.parent_nid.numpy()\n",
    "    part_loc = node_locs[part_nodes]\n",
    "    print(out_spm.shape, part_nodes.shape, part_loc.shape)\n",
    "    pickle.dump((out_spm, part_nodes, part_loc), open('reddit_part_{}.pkl'.format(i), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, part in parts.items():\n",
    "    out_spm = part.adjacency_matrix_scipy(transpose=True).tocoo()\n",
    "    row = np.expand_dims(out_spm.row, 1)\n",
    "    col = np.expand_dims(out_spm.col, 1)\n",
    "    out_mat = np.concatenate([row, col], 1)\n",
    "    print(out.mat.shape)\n",
    "    np.savetxt('reddit_part_{}.csv'.format(i), out_mat)\n",
    "    part_nodes = part.parent_nid.numpy()\n",
    "    np.savetxt('reddit_part_map_{}.txt', part_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import RedditDataset\n",
    "data = RedditDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = {'feature': data.features,\n",
    "         'label': data.labels,\n",
    "         'train_mask': data.train_mask,\n",
    "         'val_mask': data.val_mask,\n",
    "         'test_mask': data.test_mask}\n",
    "pickle.dump(ndata, open('reddit_ndata.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_nodeflow(g, node_ids, num_layers):\n",
    "    batch_size = len(node_ids)\n",
    "    expand_factor = g.number_of_nodes()\n",
    "    sampler = dgl.contrib.sampling.NeighborSampler(g, batch_size,\n",
    "            expand_factor=expand_factor, num_hops=num_layers,\n",
    "            seed_nodes=node_ids)\n",
    "    return next(iter(sampler))\n",
    "\n",
    "for i in range(num_parts):\n",
    "    print(i)\n",
    "    out_spm, part_nodes, part_loc = pickle.load(open('reddit_part_{}.pkl'.format(i), 'rb'))\n",
    "    subg = dgl.DGLGraph(out_spm, readonly=True)\n",
    "    node_ids = np.nonzero(node_locs == i)[0]\n",
    "    lnode_ids = np.nonzero(part_loc == i)[0]\n",
    "    nf = get_nodeflow(g, node_ids, 2)\n",
    "    lnf = get_nodeflow(subg, lnode_ids, 2)\n",
    "    for i in range(nf.num_layers):\n",
    "        layer_nids1 = nf.layer_parent_nid(i).detach().numpy()\n",
    "        layer_nids2 = lnf.layer_parent_nid(i)\n",
    "        layer_nids2 = part_nodes[layer_nids2]\n",
    "        assert np.all(np.sort(layer_nids1) == np.sort(layer_nids2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
