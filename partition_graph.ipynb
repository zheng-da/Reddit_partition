{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat reddit.csv | awk '{print $1, $2, 1}' > reddit.ijv\n",
    "!csrcnv reddit.ijv 6 reddit.adj 3\n",
    "!gpmetis reddit.adj 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as spsp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.loadtxt('reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm = spsp.coo_matrix((np.ones(mat.shape[0]), (mat[:,0].astype(np.int64),\n",
    "                                               mat[:,1].astype(np.int64))))\n",
    "spm = spm.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parts = 4\n",
    "part = np.loadtxt('reddit.adj.part.{}'.format(num_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetisPartition:\n",
    "    def __init__(self, spm, part, num_parts):\n",
    "        num_nodes = spm.shape[0]\n",
    "        assert num_nodes == spm.shape[1]\n",
    "        self.inner_edges = []\n",
    "        self.cut_edges = []\n",
    "        self.part_nodes = []\n",
    "        self.map2parts = part\n",
    "        \n",
    "        for i in range(num_parts):\n",
    "            print('part', i)\n",
    "            nodes = np.nonzero(part == i)[0]\n",
    "            spm_part = spm[nodes]\n",
    "            print('#nodes:', spm_part.shape)\n",
    "            print('#edges:', spm_part.nnz)\n",
    "            print('#inside edges:', np.sum(spm_part[:,nodes]))\n",
    "            self.inner_edges.append(spsp.coo_matrix(spm_part[:,nodes]))\n",
    "            spm_part1 = spm_part.transpose()\n",
    "            deg = spm_part1.dot(np.ones(spm_part1.shape[1]))\n",
    "            col_nodes = np.nonzero(deg > 0)[0]\n",
    "            all_nodes = np.unique(np.concatenate([nodes, col_nodes]))\n",
    "            print('all nodes:', len(all_nodes))\n",
    "            halo_nodes = np.setdiff1d(all_nodes, nodes)\n",
    "            print('halo nodes:', len(halo_nodes))\n",
    "            print('edge cut:', np.sum(spm_part[:,halo_nodes]))\n",
    "            self.cut_edges.append(spsp.coo_matrix(spm_part[:,halo_nodes]))\n",
    "            self.part_nodes.append((nodes, halo_nodes))\n",
    "            \n",
    "    def get_part(self, i):\n",
    "        inner_nodes, halo_nodes = self.part_nodes[i]\n",
    "        # this is symmetric\n",
    "        inner_row, inner_col = self.inner_edges[i].row, self.inner_edges[i].col\n",
    "        inner_row = inner_nodes[inner_row]\n",
    "        inner_col = inner_nodes[inner_col]\n",
    "        # this is asymmetric. Rows are inner nodes, cols are halo nodes\n",
    "        cut_row, cut_col = self.cut_edges[i].row, self.cut_edges[i].col\n",
    "        cut_row = inner_nodes[cut_row]\n",
    "        cut_col = halo_nodes[cut_col]\n",
    "        # inner edges are undirected, the cut edges has only one direction.\n",
    "        # We should make them undirected as well.\n",
    "        row = np.concatenate([inner_row, cut_row, cut_col])\n",
    "        col = np.concatenate([inner_col, cut_col, cut_row])\n",
    "        spm_part = spsp.coo_matrix((np.ones(len(row)), (row, col))).tocsr()\n",
    "        assert np.sum(spm_part[inner_nodes][:,inner_nodes]) == len(inner_row)\n",
    "        assert np.sum(spm_part[inner_nodes][:,halo_nodes]) == len(cut_row)\n",
    "        assert np.sum(spm_part[halo_nodes][:,inner_nodes]) == len(cut_row)\n",
    "        nodes = np.concatenate([inner_nodes, halo_nodes])\n",
    "        nodes = np.sort(nodes)\n",
    "        return spm_part[nodes][:,nodes], nodes, self.map2parts[nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 0\n",
      "#nodes: (58075, 232965)\n",
      "#edges: 29023404\n",
      "#inside edges: 26036992.0\n",
      "all nodes: 138336\n",
      "halo nodes: 80261\n",
      "edge cut: 2986412.0\n",
      "part 1\n",
      "#nodes: (59991, 232965)\n",
      "#edges: 46928783\n",
      "#inside edges: 41347084.0\n",
      "all nodes: 182845\n",
      "halo nodes: 122854\n",
      "edge cut: 5581699.0\n",
      "part 2\n",
      "#nodes: (56803, 232965)\n",
      "#edges: 19071516\n",
      "#inside edges: 16206226.0\n",
      "all nodes: 140812\n",
      "halo nodes: 84009\n",
      "edge cut: 2865290.0\n",
      "part 3\n",
      "#nodes: (58096, 232965)\n",
      "#edges: 19592189\n",
      "#inside edges: 17715632.0\n",
      "all nodes: 135112\n",
      "halo nodes: 77016\n",
      "edge cut: 1876557.0\n"
     ]
    }
   ],
   "source": [
    "metis_parts = MetisPartition(spm, part, num_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for i in range(num_parts):\n",
    "    part, part_nodes, part_loc = metis_parts.get_part(i)\n",
    "    pickle.dump((part, part_nodes, part_loc), open('reddit_part_{}.pkl'.format(i), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data loading.\n",
      "  NumNodes: 232965\n",
      "  NumEdges: 114615892\n",
      "  NumFeats: 602\n",
      "  NumClasses: 41\n",
      "  NumTrainingSamples: 153431\n",
      "  NumValidationSamples: 23831\n",
      "  NumTestSamples: 55703\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import RedditDataset\n",
    "data = RedditDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = {'feature': data.features,\n",
    "         'label': data.labels,\n",
    "         'train_mask': data.train_mask,\n",
    "         'val_mask': data.val_mask,\n",
    "         'test_mask': data.test_mask}\n",
    "pickle.dump(ndata, open('reddit_ndata.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
